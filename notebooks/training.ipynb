{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dheeraj/Projects/Explainable-AI-Non-EEG'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.create_dataset import DataframePrepAllMod  \n",
    "from src.constants.constants import *\n",
    "from src.model.model import ConvModel, train_cp\n",
    "from src.utils.cross_val import TrainTestSplitter\n",
    "from src.utils.utils import *\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1517588486.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    device = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "    num_epochs = args.e\n",
    "    batch_size = args.batch\n",
    "    \n",
    "    # Check if dataset file exists and create it if not\n",
    "    if not os.path.isfile(os.path.join(SAVED_DATASET_DIR, args.f)): \n",
    "        print(\"Dataset file not found!\")\n",
    "        print(\"Generating dataset file:\", args.f)\n",
    "        create_data_set = DataframePrepAllMod(\n",
    "            subjects=SUBJECTS, modals=MODALS, modals_1=MODALS_1, modals_2=MODALS_2,\n",
    "            df_hr_spo2=HR_SPO2_FILE_PATH, df_acc_temp_eda=ACC_TEMP_EDA_FILE_PATH,\n",
    "            label=LABELS, frame_rate=60, max_size=360*8\n",
    "        )\n",
    "        data_set = create_data_set.dataframe_prep_allmod()\n",
    "        data_set.to_json(os.path.join(SAVED_DATASET_DIR, args.f))\n",
    "\n",
    "    else:\n",
    "        print(\"Dataset file found!\")\n",
    "        data_set = pd.read_json(os.path.join(SAVED_DATASET_DIR, args.f))\n",
    "        if args.cross_val == 'LOSO':\n",
    "            train_df, test_df = TrainTestSplitter.train_test_loso(data_set, subject_index=args.subject_index)\n",
    "            train_filename = f\"train_data_{args.cross_val}_{args.subject_index}.pkl\"\n",
    "            test_filename = f\"test_data_{args.cross_val}_{args.subject_index}.pkl\"\n",
    "\n",
    "        else:\n",
    "            train_df, test_df = TrainTestSplitter.train_test_losego(data_set, seg_index=args.seg_index, num_seg=args.num_seg)\n",
    "            train_filename = f\"train_data_{args.cross_val}_{args.seg_index}_{args.num_seg}.pkl\"\n",
    "            test_filename = f\"test_data_{args.cross_val}_{args.seg_index}_{args.num_seg}.pkl\"\n",
    "\n",
    "        train_path = os.path.join(SAVED_DATASET_DIR, train_filename)            \n",
    "        test_path = os.path.join(SAVED_DATASET_DIR, test_filename)\n",
    "\n",
    "        save_dataframe_to_pickle(train_df, train_path) \n",
    "        save_dataframe_to_pickle(test_df, test_path)\n",
    "\n",
    "    # Prepare data for training and validation \n",
    "    train_data, train_label = dataframe_to_array(train_df)\n",
    "    train_data, train_label, val_data, val_label = preprocess_train_val(train_data, train_label)\n",
    "    print(\"train set size:\", len(train_data))\n",
    "    print(\"val set size:\", len(val_data))\n",
    "    \n",
    "\n",
    "    # Model training\n",
    "    input_shape = train_data.shape[1:]\n",
    "\n",
    "    if args.cross_val == 'LOSO':\n",
    "        model_path = os.path.join(SAVED_MODEL_DIR, f\"model_{args.cross_val}_{args.subject_index}.h5\")\n",
    "    else:\n",
    "        model_path = os.path.join(SAVED_MODEL_DIR, f\"model_{args.cross_val}_{args.seg_index}_{args.num_seg}.h5\")\n",
    "\n",
    "    model = ConvModel(input_shape=input_shape, nb_classes=len(LABELS))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "    train_cp(model, optimizer, train_data, train_label, val_data, val_label, epochs=args.e, batch_size=args.batch, model_path=model_path)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cp(model, x_train, y_train, x_val, y_val, epochs, batch_size, path):\n",
    "    \"\"\"\n",
    "    Trains the ConvModel model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (keras.Model): ConvModel model.\n",
    "    - x_train (np.ndarray): Training data.\n",
    "    - y_train (np.ndarray): Training labels.\n",
    "    - x_val (np.ndarray): Validation data.\n",
    "    - y_val (np.ndarray): Validation labels.\n",
    "    - epochs (int): Number of epochs.\n",
    "    - batch_size (int): Batch size.\n",
    "    - path (str): Path to save the model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "    - keras.callbacks.History: History of the training process.\n",
    "    \"\"\"\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path, \n",
    "                                                                monitor='val_accuracy', save_weights_only=True, \n",
    "                                                                mode='max', save_best_only=True)\n",
    "    \n",
    "    print('Training...')\n",
    "\n",
    "    history = model.model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size, \n",
    "                              callbacks=[lr_scheduler, early_stopping, model_checkpoint_callback])\n",
    "\n",
    "    # Save the model checkpoints\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    model.model.save(path)\n",
    "\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.join(os.getcwd())\n",
    "SAVE_DATASET_DIR = os.path.join(ROOT_DIR, 'data', 'created_dataset', 'processed_dataframe_allmod_8Hz.pkl')\n",
    "loaded_df = load_dataframe_from_pickle(SAVE_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = TrainTestSplitter(subjects=SUBJECTS, labels=LABELS)\n",
    "train_df, test_df = cross_val.train_test_loso(df=loaded_df, subject_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = dataframe_to_array(train_df)\n",
    "train_data, train_label, val_data, val_label = preprocess_train_val(train_data, train_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_data.shape[1:]\n",
    "model = ConvModel(input_shape=input_shape, nb_classes=len(LABELS))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_DIR = os.path.join(ROOT_DIR, 'saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(SAVED_MODEL_DIR, f\"model_LOSO_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4468, 60, 7, 1), (4468, 4), (1916, 60, 7, 1), (1916, 4))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, train_label.shape, val_data.shape, val_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 4) and (None, 7) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_cp(model, train_data, train_label, val_data, val_label, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, path\u001b[39m=\u001b[39;49mmodel_path)\n",
      "\u001b[1;32m/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m early_stopping \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(x_train, y_train, validation_data\u001b[39m=\u001b[39;49m(x_val, y_val), epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, callbacks\u001b[39m=\u001b[39;49m[lr_scheduler, early_stopping])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Save the model checkpoints\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dheeraj/Projects/Explainable-AI-Non-EEG/notebooks/training.ipynb#X31sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path):\n",
      "File \u001b[0;32m~/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filen0642nml.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1127, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1185, in compute_loss\n        return self.compiled_loss(\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/backend.py\", line 5575, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 4) and (None, 7) are incompatible\n"
     ]
    }
   ],
   "source": [
    "train_cp(model, train_data, train_label, val_data, val_label, epochs=100, batch_size=64, path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #for i in os.listdir('data/raw'):\n",
    "        #if i == \"fully_connected_patches_scale_diff_all_shapes__15_kps.pkl\":\n",
    "    test_num = 1\n",
    "    test_depth = 2\n",
    "    #test_shapes = \"2048, 2048\" #\"256, 256\" # 4096 best  \n",
    "    dataset_to_train = \"fully_connected_patches_scale_diff.pkl\"\n",
    "    #graph_file_to_test = i\n",
    "    # not required for now\n",
    "    #test_graph_type = 'fully_connected'\n",
    "    #test_node_attributes = [\"norm_descriptors\"] #\"pos, norm_pos, size, norm_size, angle, norm_angle, norm_descriptors, class_id, quadrant ,patches\"\n",
    "    #test_edge_attribute = 'scale_diff' #\"distance, scale_diff, angle_diff\"\n",
    "    #test_descriptor_type = 'hu' \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Training configuration')\n",
    "    parser.add_argument('--f', type=str, default=dataset_to_train, help=\"train dataset file name\")\n",
    "    parser.add_argument('--m', type=str, default=f'ConvModel-{dataset_to_train}', help=\"model name\")\n",
    "    parser.add_argument('--device', type=str, default='cuda', help=\"cuda / cpu\")\n",
    "    parser.add_argument('--batch', type=int, default=32, help=\"batch size\")\n",
    "    parser.add_argument('--e', type=int, default=100, help=\"number of epochs\")\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help=\"learning rate\")#1e-3, 5.e-3\n",
    "    parser.add_argument('--optimizer', type=str, default='Adam', help=\"optimizer\")#1e-3, 5.e-3\n",
    "    parser.add_argument('--model_dir', type=str, default=\"models/\", help=\"path to save model\")\n",
    "    parser.add_argument('--cross_val', type=str, default='LOSO', help=\"cross validation method (LOSO / LOSegO)\")\n",
    "    parser.add_argument('--subject_index', type=int, default=0, help=\"subject index for LOSO\")\n",
    "    parser.add_argument('--num_seg', type=int, default=1, help=\"number of segments to leave for LOSegO\")\n",
    "    parser.add_argument('--segment_index', type=int, default=0, help=\"segment index for LOSegO\")\n",
    "    #parser.add_argument('--n_train', type=int, default=140, help=\"number of samples for train set\")\n",
    "    #parser.add_argument('--n_test', type=int, default=140, help=\"number of samples for test set\")\n",
    "    #parser.add_argument('--depth', type=int, default=test_depth, help=\"depth of encoder and decoder\")\n",
    "\n",
    "    #writer = SummaryWriter()\n",
    "    #writer.add_text('Arguments', json.dumps(vars(args), indent=4))\n",
    "    main(args)\n",
    "    #writer.close()\n",
    "    print(\"Training Finished, training next model now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_losego(df)\n",
    "data = np.array([np.array(xi, dtype='float32') for xi in df_train['Data']])\n",
    "#data = data.mean(axis=1)\n",
    "\n",
    "cpdir = os.path.join(\"checkpoints\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "checkpoint_filepath = cpdir\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, \n",
    "                                                            monitor='val_accuracy', save_weights_only=True, \n",
    "                                                            mode='max', save_best_only=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.5, patience=10, min_lr=0.0001) \n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=0, mode='auto',\n",
    "                                        baseline=None, restore_best_weights=False)\n",
    "\n",
    "moddir = os.path.join(\"saved_model_new/All_modalities/LoSego/8Hz-60s/Seq_seg/\", \n",
    "                    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "batch_size = 64 #min(x_train.shape[0]/10, 16)    \n",
    "\n",
    "nb_classes = len(np.unique(y_val))\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_val = (y_val - y_val.min())/(y_val.max()-y_val.min())*(nb_classes-1)   \n",
    "\n",
    "Y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
    "Y_val = keras.utils.to_categorical(y_val, nb_classes)\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "x_val = (x_val - x_train_mean)/(x_train_std)\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_val = x_val.reshape(x_val.shape + (1,))\n",
    "x_val_std = x_val.std()\n",
    "x_val_mean = x_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    PATH: str = os.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
