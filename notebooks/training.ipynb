{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dheeraj/Projects/Explainable-AI-Non-EEG'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 14:13:35.648972: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-14 14:13:35.672839: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-14 14:13:35.672862: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-14 14:13:35.672877: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-14 14:13:35.677820: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.create_dataset import DataframePrepAllMod  \n",
    "from src.constants.constants import *\n",
    "from src.model.model import ConvModel, train_cp\n",
    "from src.utils.cross_val import TrainTestSplitter\n",
    "from src.utils.utils import *\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1517588486.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    device = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "    num_epochs = args.e\n",
    "    batch_size = args.batch\n",
    "    \n",
    "    # Check if dataset file exists and create it if not\n",
    "    if not os.path.isfile(os.path.join(SAVED_DATASET_DIR, args.f)): \n",
    "        print(\"Dataset file not found!\")\n",
    "        print(\"Generating dataset file:\", args.f)\n",
    "        create_data_set = DataframePrepAllMod(\n",
    "            subjects=SUBJECTS, modals=MODALS, modals_1=MODALS_1, modals_2=MODALS_2,\n",
    "            df_hr_spo2=HR_SPO2_FILE_PATH, df_acc_temp_eda=ACC_TEMP_EDA_FILE_PATH,\n",
    "            label=LABELS, frame_rate=60, max_size=360*8\n",
    "        )\n",
    "        data_set = create_data_set.dataframe_prep_allmod()\n",
    "        data_set.to_json(os.path.join(SAVED_DATASET_DIR, args.f))\n",
    "\n",
    "    else:\n",
    "        print(\"Dataset file found!\")\n",
    "        data_set = pd.read_json(os.path.join(SAVED_DATASET_DIR, args.f))\n",
    "        if args.cross_val == 'LOSO':\n",
    "            train_df, test_df = TrainTestSplitter.train_test_loso(data_set, subject_index=args.subject_index)\n",
    "            train_filename = f\"train_data_{args.cross_val}_{args.subject_index}.pkl\"\n",
    "            test_filename = f\"test_data_{args.cross_val}_{args.subject_index}.pkl\"\n",
    "\n",
    "        else:\n",
    "            train_df, test_df = TrainTestSplitter.train_test_losego(data_set, seg_index=args.seg_index, num_seg=args.num_seg)\n",
    "            train_filename = f\"train_data_{args.cross_val}_{args.seg_index}_{args.num_seg}.pkl\"\n",
    "            test_filename = f\"test_data_{args.cross_val}_{args.seg_index}_{args.num_seg}.pkl\"\n",
    "\n",
    "        train_path = os.path.join(SAVED_DATASET_DIR, train_filename)            \n",
    "        test_path = os.path.join(SAVED_DATASET_DIR, test_filename)\n",
    "\n",
    "        save_dataframe_to_pickle(train_df, train_path) \n",
    "        save_dataframe_to_pickle(test_df, test_path)\n",
    "\n",
    "    # Prepare data for training and validation \n",
    "    train_data, train_label = dataframe_to_array(train_df)\n",
    "    train_data, train_label, val_data, val_label = preprocess_train_val(train_data, train_label)\n",
    "    print(\"train set size:\", len(train_data))\n",
    "    print(\"val set size:\", len(val_data))\n",
    "    \n",
    "\n",
    "    # Model training\n",
    "    input_shape = train_data.shape[1:]\n",
    "\n",
    "    if args.cross_val == 'LOSO':\n",
    "        model_path = os.path.join(SAVED_MODEL_DIR, f\"model_{args.cross_val}_{args.subject_index}.h5\")\n",
    "    else:\n",
    "        model_path = os.path.join(SAVED_MODEL_DIR, f\"model_{args.cross_val}_{args.seg_index}_{args.num_seg}.h5\")\n",
    "\n",
    "    model = ConvModel(input_shape=input_shape, nb_classes=len(LABELS))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "    train_cp(model, optimizer, train_data, train_label, val_data, val_label, epochs=args.e, batch_size=args.batch, model_path=model_path)\n",
    "    print(\"Training Finished\")\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cp(model, x_train, y_train, x_val, y_val, epochs, batch_size, path):\n",
    "    \"\"\"\n",
    "    Trains the ConvModel model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (keras.Model): ConvModel model.\n",
    "    - x_train (np.ndarray): Training data.\n",
    "    - y_train (np.ndarray): Training labels.\n",
    "    - x_val (np.ndarray): Validation data.\n",
    "    - y_val (np.ndarray): Validation labels.\n",
    "    - epochs (int): Number of epochs.\n",
    "    - batch_size (int): Batch size.\n",
    "    - path (str): Path to save the model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "    - keras.callbacks.History: History of the training process.\n",
    "    \"\"\"\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path, \n",
    "                                                                monitor='val_accuracy', save_weights_only=True, \n",
    "                                                                mode='max', save_best_only=True)\n",
    "    \n",
    "    print('Training...')\n",
    "\n",
    "    history = model.model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size, \n",
    "                              callbacks=[lr_scheduler, early_stopping, model_checkpoint_callback])\n",
    "\n",
    "    # Save the model checkpoints\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    model.model.save(path)\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.join(os.getcwd())\n",
    "SAVE_DATASET_DIR = os.path.join(ROOT_DIR, 'data', 'created_dataset', 'processed_dataframe_allmod_8Hz.pkl')\n",
    "loaded_df = load_dataframe_from_pickle(SAVE_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = TrainTestSplitter(subjects=SUBJECTS, labels=LABELS)\n",
    "train_df, test_df = cross_val.train_test_loso(df=loaded_df, subject_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = np.unique(train_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = dataframe_to_array(train_df)\n",
    "train_data, train_label, val_data, val_label = preprocess_train_val(train_data, train_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_data.shape[1:]\n",
    "model = ConvModel(input_shape=input_shape, nb_classes=4)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_DIR = os.path.join(ROOT_DIR, 'saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(SAVED_MODEL_DIR, f\"model_LOSO_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 13:44:45.250766: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/dropout_2/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 4s 39ms/step - loss: 1.0650 - accuracy: 0.5998 - val_loss: 1.2451 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.9250 - accuracy: 0.6421 - val_loss: 1.1864 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.8592 - accuracy: 0.6632 - val_loss: 1.2316 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7909 - accuracy: 0.6943 - val_loss: 1.3571 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7546 - accuracy: 0.7090 - val_loss: 1.3288 - val_accuracy: 0.5616 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7402 - accuracy: 0.7146 - val_loss: 0.9145 - val_accuracy: 0.6352 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7294 - accuracy: 0.7258 - val_loss: 1.0043 - val_accuracy: 0.6106 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.7003 - accuracy: 0.7263 - val_loss: 16.8055 - val_accuracy: 0.1477 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6942 - accuracy: 0.7422 - val_loss: 9.7010 - val_accuracy: 0.1477 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6877 - accuracy: 0.7321 - val_loss: 6.1432 - val_accuracy: 0.1545 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6704 - accuracy: 0.7435 - val_loss: 9.5108 - val_accuracy: 0.1602 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6518 - accuracy: 0.7464 - val_loss: 3.0485 - val_accuracy: 0.2072 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6370 - accuracy: 0.7536 - val_loss: 0.8351 - val_accuracy: 0.6691 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6211 - accuracy: 0.7637 - val_loss: 0.6421 - val_accuracy: 0.7563 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6131 - accuracy: 0.7630 - val_loss: 2.8329 - val_accuracy: 0.2270 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6096 - accuracy: 0.7677 - val_loss: 1.0183 - val_accuracy: 0.5522 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6076 - accuracy: 0.7645 - val_loss: 0.6577 - val_accuracy: 0.7484 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6032 - accuracy: 0.7628 - val_loss: 1.1182 - val_accuracy: 0.5548 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5966 - accuracy: 0.7668 - val_loss: 4.8553 - val_accuracy: 0.1994 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5772 - accuracy: 0.7710 - val_loss: 4.0793 - val_accuracy: 0.1926 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5701 - accuracy: 0.7760 - val_loss: 1.3738 - val_accuracy: 0.4624 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5633 - accuracy: 0.7838 - val_loss: 1.1025 - val_accuracy: 0.5313 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5624 - accuracy: 0.7818 - val_loss: 0.9747 - val_accuracy: 0.6534 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5637 - accuracy: 0.7735 - val_loss: 1.0271 - val_accuracy: 0.6159 - lr: 2.5000e-04\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "train_cp(model, train_data, train_label, val_data, val_label, epochs=100, batch_size=64, path=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #for i in os.listdir('data/raw'):\n",
    "        #if i == \"fully_connected_patches_scale_diff_all_shapes__15_kps.pkl\":\n",
    "    test_num = 1\n",
    "    test_depth = 2\n",
    "    dataset_to_train = \"fully_connected_patches_scale_diff.pkl\"\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Training configuration')\n",
    "    parser.add_argument('--f', type=str, default=dataset_to_train, help=\"train dataset file name\")\n",
    "    parser.add_argument('--m', type=str, default=f'ConvModel-{dataset_to_train}', help=\"model name\")\n",
    "    parser.add_argument('--device', type=str, default='cuda', help=\"cuda / cpu\")\n",
    "    parser.add_argument('--batch', type=int, default=32, help=\"batch size\")\n",
    "    parser.add_argument('--e', type=int, default=100, help=\"number of epochs\")\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help=\"learning rate\")#1e-3, 5.e-3\n",
    "    parser.add_argument('--optimizer', type=str, default='Adam', help=\"optimizer\")#1e-3, 5.e-3\n",
    "    parser.add_argument('--model_dir', type=str, default=\"models/\", help=\"path to save model\")\n",
    "    parser.add_argument('--cross_val', type=str, default='LOSO', help=\"cross validation method (LOSO / LOSegO)\")\n",
    "    parser.add_argument('--subject_index', type=int, default=0, help=\"subject index for LOSO\")\n",
    "    parser.add_argument('--num_seg', type=int, default=1, help=\"number of segments to leave for LOSegO\")\n",
    "    parser.add_argument('--segment_index', type=int, default=0, help=\"segment index for LOSegO\")\n",
    "\n",
    "    main(args)\n",
    "    print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_losego(df)\n",
    "data = np.array([np.array(xi, dtype='float32') for xi in df_train['Data']])\n",
    "#data = data.mean(axis=1)\n",
    "\n",
    "cpdir = os.path.join(\"checkpoints\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "checkpoint_filepath = cpdir\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, \n",
    "                                                            monitor='val_accuracy', save_weights_only=True, \n",
    "                                                            mode='max', save_best_only=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.5, patience=10, min_lr=0.0001) \n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=0, mode='auto',\n",
    "                                        baseline=None, restore_best_weights=False)\n",
    "\n",
    "moddir = os.path.join(\"saved_model_new/All_modalities/LoSego/8Hz-60s/Seq_seg/\", \n",
    "                    datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "batch_size = 64 #min(x_train.shape[0]/10, 16)    \n",
    "\n",
    "nb_classes = len(np.unique(y_val))\n",
    "y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "y_val = (y_val - y_val.min())/(y_val.max()-y_val.min())*(nb_classes-1)   \n",
    "\n",
    "Y_train = keras.utils.to_categorical(y_train, nb_classes)\n",
    "Y_val = keras.utils.to_categorical(y_val, nb_classes)\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "x_val = (x_val - x_train_mean)/(x_train_std)\n",
    "x_train = x_train.reshape(x_train.shape + (1,))\n",
    "x_val = x_val.reshape(x_val.shape + (1,))\n",
    "x_val_std = x_val.std()\n",
    "x_val_mean = x_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    PATH: str = os.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
