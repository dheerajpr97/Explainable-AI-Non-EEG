{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dheeraj/Projects/Explainable-AI-Non-EEG'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 14:09:50.259449: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-15 14:09:50.281300: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-15 14:09:50.281321: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-15 14:09:50.281336: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-15 14:09:50.285453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src.dataset.create_dataset import DataframePrepAllMod  \n",
    "from src.constants.constants import *\n",
    "from src.model.model import ConvModel, train_cp\n",
    "from src.utils.cross_val import TrainTestSplitter\n",
    "from src.utils.utils import *\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1517588486.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def main(args):\n",
    "    device = \"GPU\" if tf.config.list_physical_devices('GPU') else \"CPU\"\n",
    "    num_epochs = args.e\n",
    "    batch_size = args.batch\n",
    "    \n",
    "    # Check if dataset file exists and create it if not\n",
    "    if not os.path.isfile(os.path.join(SAVED_DATASET_DIR, args.f)): \n",
    "        print(\"Dataset file not found!\")\n",
    "        print(\"Generating dataset file:\", args.f)\n",
    "        create_data_set = DataframePrepAllMod(\n",
    "            subjects=SUBJECTS, modals=MODALS, modals_1=MODALS_1, modals_2=MODALS_2,\n",
    "            df_hr_spo2=HR_SPO2_FILE_PATH, df_acc_temp_eda=ACC_TEMP_EDA_FILE_PATH,\n",
    "            label=LABELS, frame_rate=60, max_size=360*8\n",
    "        )\n",
    "        data_set = create_data_set.dataframe_prep_allmod()\n",
    "        data_set.to_json(os.path.join(SAVED_DATASET_DIR, args.f))\n",
    "\n",
    "    else:\n",
    "        print(\"Dataset file found!\")\n",
    "        data_set = pd.read_json(os.path.join(SAVED_DATASET_DIR, args.f))\n",
    "        if args.cross_val == 'LOSO':\n",
    "            train_df, test_df = TrainTestSplitter.train_test_loso(data_set, subject_index=args.subject_index)\n",
    "            train_filename = f\"train_data_{args.cross_val}_{args.subject_index}.pkl\"\n",
    "            test_filename = f\"test_data_{args.cross_val}_{args.subject_index}.pkl\"\n",
    "\n",
    "        else:\n",
    "            train_df, test_df = TrainTestSplitter.train_test_losego(data_set, seg_index=args.seg_index, num_seg=args.num_seg)\n",
    "            train_filename = f\"train_data_{args.cross_val}_{args.seg_index}_{args.num_seg}.pkl\"\n",
    "            test_filename = f\"test_data_{args.cross_val}_{args.seg_index}_{args.num_seg}.pkl\"\n",
    "\n",
    "        train_path = os.path.join(SAVED_DATASET_DIR, train_filename)            \n",
    "        test_path = os.path.join(SAVED_DATASET_DIR, test_filename)\n",
    "\n",
    "        save_dataframe_to_pickle(train_df, train_path) \n",
    "        save_dataframe_to_pickle(test_df, test_path)\n",
    "\n",
    "    # Prepare data for training and validation \n",
    "    train_data, train_label = dataframe_to_array(train_df)\n",
    "    train_data, train_label, val_data, val_label = preprocess_train_val(train_data, train_label)\n",
    "    print(\"train set size:\", len(train_data))\n",
    "    print(\"val set size:\", len(val_data))\n",
    "    \n",
    "\n",
    "    # Model training\n",
    "    input_shape = train_data.shape[1:]\n",
    "\n",
    "    if args.cross_val == 'LOSO':\n",
    "        model_path = os.path.join(SAVED_MODEL_DIR, f\"model_{args.cross_val}_{args.subject_index}.h5\")\n",
    "    else:\n",
    "        model_path = os.path.join(SAVED_MODEL_DIR, f\"model_{args.cross_val}_{args.seg_index}_{args.num_seg}.h5\")\n",
    "\n",
    "    model = ConvModel(input_shape=input_shape, nb_classes=len(LABELS))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "    train_cp(model, optimizer, train_data, train_label, val_data, val_label, epochs=args.e, batch_size=args.batch, model_path=model_path)\n",
    "    print(\"Training Finished\")\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cp(model, x_train, y_train, x_val, y_val, epochs, batch_size, path):\n",
    "    \"\"\"\n",
    "    Trains the ConvModel model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (keras.Model): ConvModel model.\n",
    "    - x_train (np.ndarray): Training data.\n",
    "    - y_train (np.ndarray): Training labels.\n",
    "    - x_val (np.ndarray): Validation data.\n",
    "    - y_val (np.ndarray): Validation labels.\n",
    "    - epochs (int): Number of epochs.\n",
    "    - batch_size (int): Batch size.\n",
    "    - path (str): Path to save the model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "    - keras.callbacks.History: History of the training process.\n",
    "    \"\"\"\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model_checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=path, \n",
    "                                                                monitor='val_accuracy', save_weights_only=True, \n",
    "                                                                mode='max', save_best_only=True)\n",
    "    \n",
    "    print('Training...')\n",
    "\n",
    "    history = model.model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size, \n",
    "                              callbacks=[lr_scheduler, early_stopping, model_checkpoint_callback])\n",
    "\n",
    "    # Save the model checkpoints\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    model.model.save(path)\n",
    "    print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.join(os.getcwd())\n",
    "SAVE_DATASET_DIR = os.path.join(ROOT_DIR, 'data', 'created_dataset', 'processed_dataframe_allmod_8Hz.pkl')\n",
    "loaded_df = load_dataframe_from_pickle(SAVE_DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_ori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[89.0, 97.0, 30.1, 0.083, 0.76, -0.87, -0.1],...</td>\n",
       "      <td>Subject_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Relax_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[71.86072423398329, 97.0, 30.8, 0.083, 0.72, ...</td>\n",
       "      <td>Subject_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Relax_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[69.0, 95.0, 31.2, 0.087, 0.14228412256267461...</td>\n",
       "      <td>Subject_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Relax_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[70.41782729805016, 95.0, 31.6, 0.087, 0.15, ...</td>\n",
       "      <td>Subject_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Relax_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[76.44289693593313, 96.0, 31.8, 0.087, 0.1554...</td>\n",
       "      <td>Subject_1</td>\n",
       "      <td>0</td>\n",
       "      <td>Relax_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>[[77.0, 94.0, 34.5, 2.3835766016713094, 0.8621...</td>\n",
       "      <td>Subject_9</td>\n",
       "      <td>3</td>\n",
       "      <td>EmotionalStress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>[[74.0, 95.0, 34.3, 2.237, -0.1770194986072419...</td>\n",
       "      <td>Subject_9</td>\n",
       "      <td>3</td>\n",
       "      <td>EmotionalStress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>[[75.0, 96.0, 34.1, 1.77, -0.17, -0.9905292479...</td>\n",
       "      <td>Subject_9</td>\n",
       "      <td>3</td>\n",
       "      <td>EmotionalStress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>[[80.0, 96.40947075208913, 34.1, 1.43998050139...</td>\n",
       "      <td>Subject_9</td>\n",
       "      <td>3</td>\n",
       "      <td>EmotionalStress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>[[76.26183844011143, 96.0, 33.9, 1.18401949860...</td>\n",
       "      <td>Subject_9</td>\n",
       "      <td>3</td>\n",
       "      <td>EmotionalStress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>840 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Data    Subject  Label  \\\n",
       "0    [[89.0, 97.0, 30.1, 0.083, 0.76, -0.87, -0.1],...  Subject_1      0   \n",
       "1    [[71.86072423398329, 97.0, 30.8, 0.083, 0.72, ...  Subject_1      0   \n",
       "2    [[69.0, 95.0, 31.2, 0.087, 0.14228412256267461...  Subject_1      0   \n",
       "3    [[70.41782729805016, 95.0, 31.6, 0.087, 0.15, ...  Subject_1      0   \n",
       "4    [[76.44289693593313, 96.0, 31.8, 0.087, 0.1554...  Subject_1      0   \n",
       "..                                                 ...        ...    ...   \n",
       "835  [[77.0, 94.0, 34.5, 2.3835766016713094, 0.8621...  Subject_9      3   \n",
       "836  [[74.0, 95.0, 34.3, 2.237, -0.1770194986072419...  Subject_9      3   \n",
       "837  [[75.0, 96.0, 34.1, 1.77, -0.17, -0.9905292479...  Subject_9      3   \n",
       "838  [[80.0, 96.40947075208913, 34.1, 1.43998050139...  Subject_9      3   \n",
       "839  [[76.26183844011143, 96.0, 33.9, 1.18401949860...  Subject_9      3   \n",
       "\n",
       "           Label_ori  \n",
       "0            Relax_1  \n",
       "1            Relax_1  \n",
       "2            Relax_1  \n",
       "3            Relax_1  \n",
       "4            Relax_1  \n",
       "..               ...  \n",
       "835  EmotionalStress  \n",
       "836  EmotionalStress  \n",
       "837  EmotionalStress  \n",
       "838  EmotionalStress  \n",
       "839  EmotionalStress  \n",
       "\n",
       "[840 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = TrainTestSplitter(subjects=SUBJECTS, labels=LABELS)\n",
    "train_df, test_df = cross_val.train_test_loso(df=loaded_df, subject_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = np.unique(train_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label = dataframe_to_array(train_df)\n",
    "train_data, train_label, val_data, val_label = preprocess_train_val(train_data, train_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_data.shape[1:]\n",
    "model = ConvModel(input_shape=input_shape, nb_classes=4)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_MODEL_DIR = os.path.join(ROOT_DIR, 'saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(SAVED_MODEL_DIR, f\"model_LOSO_0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 13:44:45.250766: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_2/dropout_2/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70/70 [==============================] - 4s 39ms/step - loss: 1.0650 - accuracy: 0.5998 - val_loss: 1.2451 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.9250 - accuracy: 0.6421 - val_loss: 1.1864 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.8592 - accuracy: 0.6632 - val_loss: 1.2316 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7909 - accuracy: 0.6943 - val_loss: 1.3571 - val_accuracy: 0.5605 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7546 - accuracy: 0.7090 - val_loss: 1.3288 - val_accuracy: 0.5616 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7402 - accuracy: 0.7146 - val_loss: 0.9145 - val_accuracy: 0.6352 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.7294 - accuracy: 0.7258 - val_loss: 1.0043 - val_accuracy: 0.6106 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.7003 - accuracy: 0.7263 - val_loss: 16.8055 - val_accuracy: 0.1477 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6942 - accuracy: 0.7422 - val_loss: 9.7010 - val_accuracy: 0.1477 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6877 - accuracy: 0.7321 - val_loss: 6.1432 - val_accuracy: 0.1545 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6704 - accuracy: 0.7435 - val_loss: 9.5108 - val_accuracy: 0.1602 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6518 - accuracy: 0.7464 - val_loss: 3.0485 - val_accuracy: 0.2072 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6370 - accuracy: 0.7536 - val_loss: 0.8351 - val_accuracy: 0.6691 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6211 - accuracy: 0.7637 - val_loss: 0.6421 - val_accuracy: 0.7563 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6131 - accuracy: 0.7630 - val_loss: 2.8329 - val_accuracy: 0.2270 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 3s 36ms/step - loss: 0.6096 - accuracy: 0.7677 - val_loss: 1.0183 - val_accuracy: 0.5522 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6076 - accuracy: 0.7645 - val_loss: 0.6577 - val_accuracy: 0.7484 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.6032 - accuracy: 0.7628 - val_loss: 1.1182 - val_accuracy: 0.5548 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5966 - accuracy: 0.7668 - val_loss: 4.8553 - val_accuracy: 0.1994 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5772 - accuracy: 0.7710 - val_loss: 4.0793 - val_accuracy: 0.1926 - lr: 2.5000e-04\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5701 - accuracy: 0.7760 - val_loss: 1.3738 - val_accuracy: 0.4624 - lr: 2.5000e-04\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5633 - accuracy: 0.7838 - val_loss: 1.1025 - val_accuracy: 0.5313 - lr: 2.5000e-04\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5624 - accuracy: 0.7818 - val_loss: 0.9747 - val_accuracy: 0.6534 - lr: 2.5000e-04\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 3s 37ms/step - loss: 0.5637 - accuracy: 0.7735 - val_loss: 1.0271 - val_accuracy: 0.6159 - lr: 2.5000e-04\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dheeraj/miniconda3/envs/xaieeg/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "train_cp(model, train_data, train_label, val_data, val_label, epochs=100, batch_size=64, path=model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
